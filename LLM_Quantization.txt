Commands: 
#token for mistral : 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
#Using Hugging face: 
Step1: 
Downloading the model: 
-Python codes: 

Use Hugging Face Transformers to download Mistral 7B:
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

-Save the model and tokenizer locally:
model.save_pretrained("./mistral-7b")
tokenizer.save_pretrained("./mistral-7b")

-Load model for quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    quantization_config=quantization_config,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
